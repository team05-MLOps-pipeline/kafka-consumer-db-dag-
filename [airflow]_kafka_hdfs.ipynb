{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP43c8Z9I/oIUzwxULNjOIk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/team05-MLOps-pipeline/kafka-consumer-db-dag-/blob/main/%5Bairflow%5D_kafka_hdfs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CzvCxSU0hh_l"
      },
      "outputs": [],
      "source": [
        "from airflow.operators.python_operator import PythonOperator\n",
        "from airflow import DAG\n",
        "from datetime import datetime, timedelta\n",
        "from confluent_kafka import Consumer, KafkaError, TopicPartition\n",
        "from hdfs import InsecureClient\n",
        "import json\n",
        "\n",
        "# DAG 설정\n",
        "default_args = {\n",
        "    'owner': 'kafka_to_hdfs_dag',\n",
        "    'depends_on_past': False,\n",
        "    'start_date': datetime(2023, 10, 23),\n",
        "    'retries': 3,\n",
        "    'retry_delay': timedelta(minutes=5),  # 5분 간격으로 실행\n",
        "}\n",
        "\n",
        "dag = DAG('kafka_to_hdfs_dag',\n",
        "          default_args=default_args,\n",
        "          catchup=False,\n",
        "          schedule_interval=timedelta(minutes=5))\n",
        "\n",
        "\n",
        "\n",
        "def kafka_to_hdfs():\n",
        "\n",
        "    topic_name = \"euntest\"\n",
        "    partition = 0\n",
        "\n",
        "    kafka_conf = {\n",
        "        'bootstrap.servers': 'ip:9092',\n",
        "        'group.id': 'devinu',\n",
        "        'auto.offset.reset': 'latest'\n",
        "    }\n",
        "\n",
        "    consumer = Consumer(kafka_conf)\n",
        "    consumer.subscribe([topic_name])\n",
        "\n",
        "    # Hadoop 클라이언트 설정\n",
        "    hdfs_client = InsecureClient('http://ip9870', user='root')\n",
        "\n",
        "    now = datetime.now()\n",
        "\n",
        "    file_name = now.strftime(\"%Y%m%d%H%M%S\") + '.txt'\n",
        "    hdfs_path = '/test/' + file_name\n",
        "\n",
        "    data_to_save = []\n",
        "    partition_lag = 0\n",
        "    topic = TopicPartition(topic_name, partition)\n",
        "    consumer.assign([topic])\n",
        "\n",
        "    while True:\n",
        "        committed = consumer.committed([topic])[0].offset\n",
        "        last_offset = consumer.get_watermark_offsets(topic)[1]\n",
        "        partition_lag = last_offset - committed\n",
        "\n",
        "        if partition_lag <= 1:\n",
        "            break\n",
        "\n",
        "        msg = consumer.poll(timeout=10)\n",
        "\n",
        "        if msg is None:\n",
        "            print(\"No message received.\")\n",
        "            # break\n",
        "            continue\n",
        "\n",
        "        print(last_offset, committed, partition_lag)\n",
        "\n",
        "        if msg.error():\n",
        "            # raise KafkaException(msg.error())\n",
        "            pass\n",
        "        else:\n",
        "            value = msg.value().decode('unicode_escape')\n",
        "            data = json.loads(value)\n",
        "\n",
        "            # print(data)\n",
        "            data_to_save.append(data)\n",
        "\n",
        "    if data_to_save:\n",
        "        with hdfs_client.write(hdfs_path, encoding='utf-8', overwrite=True) as writer:\n",
        "            for data in data_to_save:\n",
        "                try:\n",
        "                    # data를 JSON 문자열로 직렬화\n",
        "                    data_json = json.dumps(data, ensure_ascii=False)\n",
        "                    writer.write(data_json + '\\n')\n",
        "                except Exception as e:\n",
        "                    print(f\"An error occurred: {str(e)}\")\n",
        "\n",
        "        print(f\"Saved  messages to HDFS\")\n",
        "\n",
        "\n",
        "# Airflow PythonOperator를 사용하여 위에서 정의한 함수를 실행\n",
        "kafka_to_hdfs_task = PythonOperator(\n",
        "    task_id='kafka_to_hdfs_task',\n",
        "    python_callable=kafka_to_hdfs,\n",
        "    dag=dag\n",
        ")"
      ]
    }
  ]
}